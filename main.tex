\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{fancyvrb}
\usepackage{import}
\usepackage{float}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{mathtools}
\usepackage{transparent}
\usetikzlibrary{shapes.geometric,fit}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\newtheorem{theorem}{Theorem}

% \theoremstyle{lemma}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}

\theoremstyle{definition}
\newtheorem{statement}{Statement}
\newtheorem*{definition}{Definition}
\newtheorem{claim}{Claim}

\newcommand{\contra}{\Rightarrow\!\Leftarrow}
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zeq}{\mathbb{Z}_{\geq 0}}
\newcommand{\Zg}{\mathbb{Z}_{>0}}
\newcommand{\Req}{\mathbb{R}_{\geq 0}}
\newcommand{\Rg}{\mathbb{R}_{>0}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Ha}{\mathbb{H}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lpnorm}[2]{\left|\left|{#1}\right|\right|_{L^{#2}}}
\DeclareMathOperator{\ima}{im}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\real}{Re}
\DeclareMathOperator{\imag}{Im}
\DeclareMathOperator{\diver}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\inter}{int}
\DeclareMathOperator{\Dr}{Dr}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\res}{res}
\allowdisplaybreaks

\newcommand{\incfig}[1]{\input{./figures/#1.pdf_tex}}
\graphicspath{ {./figures/} }

\title{Estimating Fractional Brownian Motion with Noise}
\author{}
\date{\today}

\begin{document}

\maketitle

Suppose we have some fBM with stochastic volatility, \(X_t\), and some noisy process that we are able to observe: \(Y_t = X_t + \rho Z_t\) where \(\rho > 0\) and \((Z_t)_{t \geq 0}\) are i.i.d.\ \(N(0,1)\) variables. We would like to be able to extract the signal from the noise, and to somehow estimate both \(H\) and \(\sigma\). To do so, we will consider weighted averages of \(Y_t\) at different time-points to eliminate the noise. In particular, we will take the following:

\begin{definition}
  For \(g: [0,1] \rightarrow \R\), some constant \(\theta\), and some stochastic process \(W_t\), put
  \begin{gather}
    k_n = \frac{n^\kappa}{\theta} \\
    g^n_j = g\left( \frac{j}{k_n} \right) \\
    \overline{W}(g)^n_i = \sum_{j=1}^{k_n-1}g^n_j\left( W_{\frac{i+j-1}{n}} - W_{\frac{i+j-2}{n}} \right) = \sum_{j=1}^{k_n-1}g^n_j\Delta_{i+j-1}^n W \\
    \widehat{W}(g)^n_i = \sum_{j=1}^{k_n}\left(g^n_j - g^n_{j-1}\right)^2\left(\Delta_{i+j-1}^n W\right)^2 = \sum_{j=1}^{k_n}\left(\Delta g^n_j\right)^2\left(\Delta_{i+j-1}^n W\right)^2
  \end{gather}
  Morally, the latter two expressions are \textit{averages} of the increments of the stochastic process, considered to try and smooth out the random noise.

  Finally, let the autocovariance of increments of fBM (called \(B_t^H\)) be asymptotically
  \begin{equation}
    \E\left[ \left( (B_{t+h}^H - B_t^H)(B_{t+(r+1)h}^H - B_{t+rh}^H) \right) \right] \sim h^{2H}K_H(r)
  \end{equation}
  and take
  \begin{equation}
    \Gamma_r^H = \frac{K_H(r)}{K_H} =
    \begin{cases}
      1 & r = 0 \\
      \frac{1}{2}\left| (r+1)^{2H} - 2r^{2H} + (r-1)^{2H}\right| & r \geq 1 \\
    \end{cases}
  \end{equation}
  where \(K_H(0)\), the variance of an increment, will be abbreviated to \(K_H\).

  Notationally, if we say that a random variable has some size \(f(n)\), then all \(L^p\) norms are bounded by \(f(n)\).
\end{definition}

We will ultimately show the following theorem:
\begin{theorem}
  Given some fBM with measurement error,
  \begin{equation}
    Y_t = X_t + \rho Z_t
  \end{equation}
  where
  \begin{equation}
    X_t = X_0 + A_t + B_t^H = X_0 + \int_0^tb_sds + K_H^{-\frac{1}{2}}\int_0^t(t-s)^{H - \frac{1}{2}}\sigma_sdB_s
  \end{equation}
  if we have the following conditions, % TODO: Explain what K_H is?
  \begin{enumerate}
    \item \(f: \R^L \rightarrow \R\) is \(C^2\) with all partial derivatives up to order 2 of at most polynomial growth.
    \item \(g: [0,1] \rightarrow \R\) is \(C^2\).
    \item \(b, \sigma\) are of size 1 (that is, \(||b_s||_{L_p}, ||\sigma_s||_{L_p}\) are bounded) and adapted.
    \item \(\sigma\) is \(L^2\)-continuous.
    \item \(\kappa \in \left(\frac{2H}{2H+1}, 1\right)\).
  \end{enumerate}
  then we get the following convergence:
  \begin{equation}
    V(g)^{n,f}_T(Y) = \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor}f\left( \frac{\overline{Y}(g)^n_i}{\left( k_n/n \right)^H}, \frac{\widehat{Y}(g)^n_i}{\left( k_n/n \right)^{2H}} \right) \overset{\mathbb{P}}{\rightarrow} \int_0^T \mu_f\left( \sigma_s^2\eta\left( g \right), \Theta^2\rho^2 \int_0^1g'(r)^2dr \right)ds
  \end{equation}
  where
  \begin{gather}
    \Theta =
    \begin{cases}
      0 & \kappa \neq \frac{2H}{2H+1} \\
      \theta &  \kappa = \frac{2H}{2H+1}
    \end{cases} \\
    \eta(g) = 2H\int_0^1g(x)\left(g(1)(1-x)^{2H-1}dx + \int_x^1(y-x)^{2H-1}g'(y)dy\right)dx
  \end{gather}
\end{theorem}
Unless otherwise mentioned, the above conditions are assumed in the following lemmas and propositions.

To motivate the normalization term in \(V(g)_T^{n,f}(Y)\), we will first compute a bound on the second moment of an averaged fBM with Hurst parameter \(H\):
\begin{prop}
  For \(B^H_t = \int_0^t(t-s)^{H - \frac{1}{2}}\sigma_sdB_s\),
  \begin{equation}
    \E\left[ \left(\overline{B^H}(g)^n_i\right)^2 \right]^{\frac{1}{2}} \leq C\left( \frac{k_n}{n} \right)^H
  \end{equation}
\end{prop}

\begin{proof}
  This is a relatively straightforward computation:
  \begin{align}
    \phantom{\E\left[ \left(\overline{B^H}(g)^n_i\right)^2 \right]}
    &\begin{aligned}
      \mathllap{\E\left[ \left(\overline{B^H}(g)^n_i\right)^2 \right]} &= \E\left[ \left( \sum_{j=1}^{k_n-1} g^n_j \Delta_{i+j-1}^nB^H \right)^2 \right] \\
                                                                     &= \E\left[ \sum_{j=1}^{k_n-1} (g^n_j\Delta_{i+j-1}^nB^H)^2 + 2\sum_{j=1}^{k_n-2}\sum_{l=j+1}^{k_n-1} g^n_jg^n_l\Delta_{i+j-1}^nB^H\Delta_{i+l-1}^nB^H \right]
    \end{aligned} \\
    \intertext{Abbreviating \(t = \frac{i+j-2}{n}, h = \frac{1}{n}, r = l-j\),}
    &\begin{aligned}
      \mathllap{} &= \sum_{j=1}^{k_n-1} (g^n_j)^2 \E\left[ \int_0^t \left[ \left( t + h - s \right)^{H-\frac{1}{2}} - \left( t - s \right)^{H - \frac{1}{2}}_+ \right]\sigma_s dB_s \right]^2 \\ &\hspace*{0.5cm}+2\sum_{j=1}^{k_n-2}\sum_{l=j+1}^{k_n-1} g^n_j g^n_l \E \left[ \int_0^t \left[ \left( t + h - s \right)^{H-\frac{1}{2}} - \left( t - s \right)^{H - \frac{1}{2}}_+ \right]\sigma_s dB_s \right. \\ &\hspace*{4.1cm} \left. \cdot \int_0^t \left[ \left( t + (r+1)h - s \right)^{H-\frac{1}{2}} - \left( t + rh - s \right)^{H - \frac{1}{2}}_+ \right]\sigma_s dB_s \vphantom{\int}\right] % Genuinely a terrible LaTeX hack but whatever lmao godspeed to whoever has to read this next
    \end{aligned}
    \intertext{Consider the expectations in a single summand, which are just the autocovariances of fBM increments, known asymptotically to be \(\sim h^{2H}K_H(r)\) (in the case of the square, take \(r = 0\)), so}
    &\begin{aligned}
      \mathllap{} &\sim \sum_{j=1}^{k_n-1} (g^n_j)^2 h^{2H}K_H + 2\sum_{j=1}^{k_n-2}\sum_{l=j+1}^{k_n-1} g^n_j g^n_l h^{2H}K_H\Gamma_{r}^H \\
                  &= h^{2H}K_H\left[ \sum_{j=1}^{k_n-1} (g^n_j)^2 + 2\sum_{j=1}^{k_n-2}\sum_{l=j+1}^{k_n-1} g^n_j g^n_l \Gamma_r^H \right]
    \end{aligned}
    \intertext{Recall that \(g\) is bounded, so this is bounded as}
    &\begin{aligned}
      \mathllap{} &\leq Ch^{2H}\left[ \sum_{j=1}^{k_n-1} 1 + \sum_{j=1}^{k_n-2}\sum_{l=j+1}^{k_n-1} \left( (r+1)^{2H} - 2r^{2H} + (r-1)^{2H} \right) \right]
    \end{aligned}
    \intertext{Reindexing, and recalling \(h = \frac{1}{n}, r = l - j\),}
    &\begin{aligned}
      \mathllap{} &= Cn^{-2H}\left[ k_n-1 + \sum_{j=1}^{k_n-2}\sum_{r=1}^{k_n-1-j} \left( (r+1)^{2H} - 2r^{2H} + (r-1)^{2H} \right) \right]
    \end{aligned}
    \intertext{The inner sum telescopes to \(-1 - (k_n - 1-j)^{2H} + (k_n-j)^{2H}\), which means the entire sum telescopes to}
    &\begin{aligned}
      \mathllap{} &= Cn^{-2H}\left[ k_n-1 + -(k_n - 2) + (k_n - 1)^{2H} - 1^{2H}\right] \\
                  &= Cn^{-2H}(k_n-1)^{2H}
    \end{aligned}
  \end{align}
  Taking square roots gets us what we wanted.
\end{proof}

The next thing to consider is the drift in the fBM: \(X_0 + A_0 = X_0 + \int_0^tb_sds\); we will show that in the end this does not matter since \(b_s\) is bounded, so we can safely work with only the fBM part of \(X_t\), as long as \(k_n\), the amount of elements we are averaging over, does not grow too fast.

\begin{definition}
  \(U_t\) will be \(Y_t\) with the drift removed, namely
  \begin{equation}
    U_t = Y_t - X_0 - A_0 = B_t^H + \rho Z_t
  \end{equation}
\end{definition}

\begin{lemma}
  If \(\frac{2H}{2H+1} < \kappa < 1\),
  \begin{equation}
    \lim_{n \rightarrow \infty} \E\left[ \left| V(g)^{n,f}_T(Y) - V(g)^{n,f}_T(U) \right| \right] = 0
    % \lim_{n \rightarrow \infty}\E\left[ \left| \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor} \left[ f\left( \frac{\overline{Y}(g)^n_i}{\left( k_n/n \right)^H}, \frac{\widehat{Y}(g)^n_i}{\left( k_n/n \right)^{2H}} \right) - f\left( \frac{\overline{U}(g)^n_i}{\left( k_n/n \right)^H}, \frac{\widehat{U}(g)^n_i}{\left( k_n/n \right)^{2H}} \right) \right] \right| \right] = 0
  \end{equation}
\end{lemma}

\begin{proof}
  Recalling the definition of the variation, the difference is really
  \begin{equation}
    \E\left[ \left| \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor} \left[ f\left( \frac{\overline{Y}(g)^n_i}{\left( k_n/n \right)^H}, \frac{\widehat{Y}(g)^n_i}{\left( k_n/n \right)^{2H}} \right) - f\left( \frac{\overline{U}(g)^n_i}{\left( k_n/n \right)^H}, \frac{\widehat{U}(g)^n_i}{\left( k_n/n \right)^{2H}} \right) \right] \right| \right]
  \end{equation}
  Applying MVT to the difference, we have that this becomes
  \begin{equation}
    \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor} \begin{pmatrix} \partial_1 f(\xi^n_{1,i}) & \partial_2 f(\xi^n_{2,i}) \end{pmatrix} \cdot \begin{pmatrix} \frac{\overline{Y}(g)^n_i - \overline{U}(g)^n_i}{(k_n/n)^H} & \frac{\widehat{Y}(g)^n_i - \widehat{U}(g)^n_i}{(k_n/n)^{2H}} \end{pmatrix}
  \end{equation}
  Now, by assumption each partial is bounded here, so we only care about the differences. Further, \(\E[|\Delta_{i+j-1}^n A|] = \E\left[\left|\int_{\frac{i+j-2}{n}}^{\frac{i+j-1}{n}}b_sds\right|\right] \leq Cn^{-1}\) by assumption, so \(\E[|\overline{A}(g)^n_i|] \leq \sum_{j=1}^{k_n-1}Cn^{-1} \leq \frac{k_n}{n}\); thus,
  \begin{equation}
    \E\left[\left|\frac{\overline{Y}(g)^n_i - \overline{U}(g)^n_i}{(k_n/n)^H}\right|\right] = \E\left[\left|\frac{\overline{A}(g)^n_i}{(k_n/n)^H} \right|\right] \leq \left(\frac{k_n}{n}\right)^{1 - H}
  \end{equation}
  which \(\rightarrow 0\) as \(\kappa < 1\). The other difference is a little trickier:
  \begin{align}
    \E\left[\left|\frac{\widehat{Y}(g)^n_i - \widehat{U}(g)^n_i}{(k_n/n)^{2H}}\right|\right] &= \E\left[\left|\frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n}(\Delta g^n_i)^2\left( (\Delta^n_{i+j-1}Y)^2 - (\Delta^n_{i+j-1}U)^2 \right)\right|\right] \\
                                                                                             &\leq \frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n}(\Delta g^n_i)^2\left( \E[|(\Delta^n_{i+j-1}A)^2|] + 2\E[|\Delta^n_{i+j-1}A\Delta^n_{i+j-1}U|] \right)
                                                                                             \intertext{From above, we have that \(\E[|(\Delta^n_{i+j-1}A)^2|] \leq Cn^{-2}\). Remembering that \(\Delta^n_{i+j-1}\rho Z \sim N(0, \rho^2)\), H\"older and Minkowski yield that that}
  \E[|\Delta^n_{i+j-1}A\Delta^n_{i+j-1}U|] &\leq \E[|\Delta^n_{i+j-1}A|^2]^{\frac{1}{2}}(\E[|\Delta^n_{i+j-1}B^H|^2]^{\frac{1}{2}} + \E[|\Delta^n_{i+j-1}\rho Z|^2]^{\frac{1}{2}}) \\
                                           &\leq C_1n^{-1}\left( \left(\frac{k_n}{n}\right)^H + C_2 \right) \leq Cn^{-1}
                                           \intertext{For sufficiently large \(n\), since \(\kappa < 1\). Applying MVT and noting that \(g'\) is bounded by assumption, we have that the expectation is bounded as follows:}
  \E\left[\left|\frac{\widehat{Y}(g)^n_i - \widehat{U}(g)^n_i}{(k_n/n)^{2H}}\right|\right] &\leq \frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n} \left(\frac{g'\left( \xi^n_i \right)}{k_n}\right)^2Cn^{-1} \\
                                                                                           &\leq \frac{n^{2H-1}}{k_n^{2H+1}}
  \end{align}
  which, since \(\kappa > \frac{2H}{2H + 1} > \frac{2H-1}{2H+1}\), \(\rightarrow 0\) as \(n \rightarrow \infty\). To conclude, the original expectation is bounded by
  \begin{equation}
    \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor}C\left( \E\left[\left|\frac{\overline{Y}(g)^n_i - \overline{U}(g)^n_i}{(k_n/n)^H}\right|\right] + \E\left[\left|\frac{\widehat{Y}(g)^n_i - \widehat{U}(g)^n_i}{(k_n/n)^{2H}}\right|\right]  \right) \rightarrow 0
  \end{equation}
  % TODO: Fully justify bounding the first partials of f; not hard just annoying
\end{proof}

Note that the lower bound on \(\kappa\) is not the most parsimonious choice possible just examining the previous proof; the next step we take is to justify a lower bound of \(\frac{2H}{2H+1}\) rather than \(\frac{2H-1}{2H+1}\). Considering \(\widehat{Y}(g)^n_i\) and the normalization we have selected, we can now restrict the growth rate of \(k_n\) from below to keep \(\widehat{Y}(g)^n_i\) finite.

\begin{lemma}
  If \(k \geq \frac{2H}{2H+1}\), \(\limsup_{n \rightarrow \infty}\lpnorm{\frac{\widehat{U}(g)^n_i}{(k_n/n)^{2H}}}{p} < \infty\) as \(n \rightarrow \infty\). Further, if \(k > \frac{2H}{2H+1}\), \(\lpnorm{\frac{\widehat{U}(g)^n_i}{(k_n/n)^{2H}}}{p} \rightarrow 0\).
\end{lemma}

\begin{proof}
  First consider the quantity \(\widehat{U}(g)^n_i = \widehat{B^H}(g)^n_i + \widehat{\rho Z}(g)^n_i\). Then, Minkowski yields that
  \begin{equation}
    \left|\lpnorm{\widehat{B^H}(g)^n_i}{p} - \lpnorm{\widehat{ \rho Z}(g)^n_i}{p}\right| \leq \lpnorm{\widehat{U}(g)^n_i}{p} \leq \lpnorm{\widehat{B^H}(g)^n_i}{p} + \lpnorm{\widehat{ \rho Z}(g)^n_i}{p}
  \end{equation}

  However, as \(n \rightarrow \infty\), \(\lpnorm{\widehat{B^H}(g)^n_i}{p} \leq \sum_{j=1}^{k_n}|\Delta g^n_j|\lpnorm{(\Delta^n_{i+j-1}B^H)^2}{p} \rightarrow 0\) since fBM is almost surely continuous, so in the limit, we are only worried about \(\lpnorm{\widehat{\rho Z}(g)^n_i}{p}\). However,
  \begin{align}
    \lpnorm{\frac{\widehat{\rho Z}(g)^n_i}{(k_n/n)^{2H}}}{p} &\leq \frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n}\left( \Delta g^n_j \right)^2\lpnorm{(\Delta_{i+j-1}^n \rho Z)^2}{p} \\
                                                             &= \frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n} \left(\frac{g'\left( \xi^n_i \right)}{k_n}\right)^2 \lpnorm{(\Delta_{i+j-1}^n \rho Z)^2}{p}
                                                             \intertext{where the last equality comes from applying MVT for some \(\xi^n_i \in \left( \frac{j-1}{k_n},\frac{j}{k_n} \right)\). Now, since \(\Delta_{i+j-1}^n \rho Z \sim N(0, \rho^2\), the norm is some constant varying with \(p\), but not \(n\), so the above is bounded by}
                                                             &\leq C_p\frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n}\left( \frac{g'(\xi^n_i)}{k_n} \right)^2 \\
                                                             &\leq C_p\frac{n^{2H}}{k_n^{2H}}\sum_{j=1}^{k_n}\left( \frac{C}{k_n} \right)^2 = C_p\frac{n^{2H}}{k_n^{2H+1}}
  \end{align}
  Since \(k_n^{2H+1} = \frac{n^{\kappa(2H + 1)}}{\theta^{2H+1}}\), if \(\kappa > \frac{2H}{2H+1}\), the above bound \(\rightarrow 0\) as \(n \rightarrow \infty\). If \(k_n = \frac{2H}{2H+1}\), exactly, this bound remains as \(C_p\).
\end{proof}

The problematic part of working with fBM here is that the domain of integration for intervals in the stochastic integral stretches all the way from \(0\), since the integrand is dependent on the upper bound of integration. In normal BM (i.e.\ with \(H = \frac{1}{2}\)), we have that an increment \(B_{t_1} - B_{t_2} = \int_{t_2}^{t_1} dB_s\), but in fBM,
\[
  B^H_{t_1} - B^H_{t_2} = \int_0^{t_1} \left[(t_1 - s)^{H - \frac{1}{2}} - (t_2 - s)_+^{H - \frac{1}{2}} \right]dB_s
\]

What we would like to be able to do is to truncate this interval arbitrarily close to where we begin averaging, namely at \(\frac{i}{n}\) in the case of \(\overline{U}(g)^n_i\). To this purpose, we make the following definitions and prove the following lemma:

\begin{definition}
  A \textit{truncated} increment of a fBM \(B^H_t\) is defined as
  \begin{equation}
    \Delta_{i+j-1}^{n,\epsilon} B^H_t =  \int_{\frac{i}{n}-\epsilon}^{\frac{i+j-1}{n}} \left[ \left( \frac{i+j-1}{n} -s \right)^{H - \frac{1}{2}} - \left( \frac{i+j-1}{n} -s \right)^{H - \frac{1}{2}} \right]\sigma_s dB_s
  \end{equation}

  Similarly, we truncate \(U_t\) as follows:
  \begin{equation}
    \Delta_{i+j-1}^{n,\epsilon} U_t = \Delta_{i+j-1}^{n,\epsilon} B^H_t + \Delta_{i+j-1}^n \rho Z
  \end{equation}
\end{definition}

In order for this truncation to be a reasonable endeavor, we need the error we are making to be small; the amount which we discard must vanish in the limit if we take \(\epsilon \rightarrow 0\).

\begin{lemma}
  \begin{equation}
    \limsup_{n \rightarrow \infty} \E \left[ \left| V(g)^{n,f}_T(Y) - \frac{1}{n}\sum_{i=\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} f\left( \frac{\overline{U}(g)^{n,\epsilon}_i}{(k_n/n)^H}, \frac{\widehat{U}(g)^{n,\epsilon}_i}{(k_n/n)^{2H}} \right) \right| \right] < C\epsilon
  \end{equation}
\end{lemma}

\begin{proof}
  % TODO
\end{proof}

\begin{lemma}
  \begin{align}
    \phantom{}
    \begin{aligned}
      &\lim_{\epsilon \rightarrow 0}\lim_{n \rightarrow \infty}\E \left[ \left| \frac{1}{n}\sum_{i=\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} \left[ f\left( \frac{\overline{U}(g)^{n,\epsilon}_i}{(k_n/n)^H}, \frac{\widehat{U}(g)^{n,\epsilon}_i}{(k_n/n)^{2H}} \right) \right. \right. \right. \\
      & \hspace*{2cm} - \left. \left. \left. f\left( \frac{\sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i + \rho\overline{Z}(g)^n_i}{(k_n/n)^{H}}, \frac{\rho^2\widehat{Z}(g)^n_i}{(k_n/n)^{2H}}\right) \right] \right| \right] = 0
    \end{aligned}
  \end{align}
\end{lemma}

\begin{proof}

\end{proof}

Now, we are able to center to the conditional expectation, with the ultimate goal of computing an explicit form:
\begin{lemma}
  \begin{align}
    \phantom{}
    &\begin{aligned}
      \mathllap{} &\frac{1}{n}\sum_{i = \left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} \left[ \vphantom{\E \left[ f\left( \frac{\sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i + \rho\overline{Z}(g)^n_i}{(k_n/n)^{2H}}, \frac{\rho^2\widehat{Z}(g)^n_i}{(k_n/n)^H}\right) \right] } f\left( \frac{\sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i + \rho\overline{Z}(g)^n_i}{(k_n/n)^{2H}}, \frac{\rho^2\widehat{Z}(g)^n_i}{(k_n/n)^H}\right) \right. \\ &\hspace*{2.1cm} - \left. \E\left[ f\left( \frac{\sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i + \rho\overline{Z}(g)^n_i}{(k_n/n)^{2H}}, \frac{\rho^2\widehat{Z}(g)^n_i}{(k_n/n)^H}\right) \big| \mathcal{F}_{\frac{i-1}{n}} \right] \vphantom{\frac{1}{n}\sum_{i = \left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor}} \right] \overset{\mathbb{P}}{\rightarrow} 0
    \end{aligned}
  \end{align}
\end{lemma}

\begin{proof}

\end{proof}

\begin{definition}
  Define
  \begin{equation}
    \mu_f(v_1, v_2) = \E\left[ f(\sqrt{v_1}Z_1 + \sqrt{v_2}Z_2, 2v^2) \right]
  \end{equation}
  where \(Z_1, Z_2 \sim N(0,1)\) and are i.i.d.
\end{definition}

\begin{lemma}
  \begin{align}
    \phantom{}
    &\begin{aligned}
      \mathllap{} &\lim_{n \rightarrow \infty} \E\left[ f\left( \frac{\sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i + \rho\overline{Z}(g)^n_i}{(k_n/n)^{2H}}, \frac{\rho^2\widehat{Z}(g)^n_i}{(k_n/n)^H}\right) \big| \mathcal{F}_{\frac{i-1}{n}} \right] \\
                  &\hspace*{2cm} \rightarrow \mu_f\left( \frac{\sigma^2_{\frac{i}{n}-\epsilon}}{(k_n/n)^{2H}}\sum_{j,l = 1}^{k_n-1} g^n_jg^n_l\Gamma^H_{|j-l|}, \frac{\rho^2}{(k_n/n)^{2H}}\sum_{j=1}^{k_n}(\Delta g^n_j)^2 \right)
    \end{aligned}
  \end{align}
\end{lemma}

\begin{proof}

\end{proof}

\begin{lemma}
  \begin{align}
    \phantom{}
    &\begin{aligned}
      \mathllap{} &\lim_{\epsilon \rightarrow 0}\E\left[ \left| \frac{1}{n}\sum_{i = \left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor}\mu_f\left( \frac{\sigma^2_{\frac{i}{n}-\epsilon}}{(k_n/n)^{2H}}\sum_{j,l = 1}^{k_n-1} g^n_jg^n_l\Gamma^H_{|j-l|}, \frac{\rho^2}{(k_n/n)^{2H}}\sum_{j=1}^{k_n}(\Delta g^n_j)^2 \right) \right.\right. \\
                  &\hspace*{2cm} \left.\left. - \int_0^T\mu_f\left(\sigma_{s}^2\mu_f(\nu(g), \Theta^2\rho^2\int_0^1g'(r)^2dr)\right) \vphantom{\E\left[ \left| \frac{1}{n}\sum_{i = \left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor}\mu_f\left( \frac{\sigma^2_{\frac{i}{n}-\epsilon}}{(k_n/n)^{2H}}\sum_{j,l = 1}^{k_n-1} g^n_jg^n_l\Gamma^H_{|j-l|}, \frac{\rho^2}{(k_n/n)^{2H}}\sum_{j=1}^{k_n}(\Delta g^n_j)^2 \right) \right.\right.} \right|\right] = 0
      \end{aligned}
  \end{align}
\end{lemma}

\begin{proof}

\end{proof}

The next theorem is just putting steps together. Restating it from before,
\begin{theorem}
  Given some fBM with measurement error,
  \begin{equation}
    Y_t = X_t + \rho Z_t
  \end{equation}
  where
  \begin{equation}
    X_t = X_0 + A_t + B_t^H = X_0 + \int_0^tb_sds + K_H^{-\frac{1}{2}}\int_0^t(t-s)^{H - \frac{1}{2}}\sigma_sdB_s
  \end{equation}
  if we have the following conditions, % TODO: Explain what K_H is?
  \begin{enumerate}
    \item \(f: \R^L \rightarrow \R\) is \(C^2\) with all partial derivatives up to order 2 of at most polynomial growth.
    \item \(g: [0,1] \rightarrow \R\) is piecewise \(C^2\).
    \item \(b, \sigma\) are of size 1 (that is, \(||b_s||_{L_p}, ||\sigma_s||_{L_p}\) are bounded) and adapted.
    \item \(\sigma\) is \(L^2\)-continuous.
    \item \(\kappa \in \left(\frac{2H}{2H+1}, 1\right)\).
  \end{enumerate}
  then we get the following convergence:
  \begin{equation}
    V(g)^{n,f}_T(Y) = \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor}f\left( \frac{\overline{Y}(g)^n_i}{\left( k_n/n \right)^H}, \frac{\widehat{Y}(g)^n_i}{\left( k_n/n \right)^{2H}} \right) \overset{\mathbb{P}}{\rightarrow} \int_0^T \mu_f\left( \sigma_s^2\eta\left( g \right), \Theta^2\rho^2 \int_0^1g'(r)^2dr \right)ds
  \end{equation}
  where
  \begin{gather}
    \Theta =
    \begin{cases}
      0 & \kappa \neq \frac{2H}{2H+1} \\
      \theta &  \kappa = \frac{2H}{2H+1}
    \end{cases} \\
    \eta(g) = 2H\int_0^1g(x)\left(g(1)(1-x)^{2H-1}dx + \int_x^1(y-x)^{2H-1}g'(y)dy\right)dx
  \end{gather}
\end{theorem}

\begin{proof}

\end{proof}

% ----------------------------------------------------------------------------------------------------------------------
\pagebreak
DRAFTS AND SNIPPETS: NOT FINAL OR EVEN COHERENT!

Truncation is taken as follows:
\begin{equation}
  \Delta_{i+j-1}^{n,\epsilon} Y =  \int_{\frac{i}{n}-\epsilon}^{\frac{i+j-1}{n}} \left[ \left( \frac{i+j-1}{n} -s \right)^{H - \frac{1}{2}} - \left( \frac{i+j-1}{n} -s \right)^{H - \frac{1}{2}} \right]\sigma_s dB_s + \rho\Delta_{i+j-1}^n Z
\end{equation}
At some point, I may refer to the fBM part here as \(X\).

Similarly,
\begin{gather}
  \overline{Y}(g)^{n,\epsilon}_i = \sum_{j=1}^{k_n-1} g^n_j \Delta_{i+j-1}^{n,\epsilon} Y \\
  \widehat{Y}(g)^{n,\epsilon}_i = \sum_{j=1}^{k_n-1} (\Delta g^n_j)^2 (\Delta_{i+j-1}^{n,\epsilon} Y)^2 \\
  V(g)^{n,f}_T(Y) = \frac{1}{n}\sum_{i=1}^{\left\lfloor nT \right\rfloor} f\left( \frac{\overline{Y}(g)^n_i}{\nu_H}, \frac{\widehat{Y}(g)^n_i}{\nu_H} \right)
\end{gather}
where \(\nu_H\) is the appropriate normalization term, dependent on \(H\).

We want to show that
\[
  \limsup_{n \rightarrow \infty} \E \left[ \left| V(g)^{n,f}_T(Y) - \frac{1}{n}\sum_{i=\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} f\left( \frac{\overline{Y}(g)^{n,\epsilon}_i}{\nu_H}, \frac{\widehat{Y}(g)^{n,\epsilon}_i}{\nu_H} \right) \right| \right] < C\epsilon
\]

The difference is composed of two parts:
\begin{gather}
  \frac{1}{n}\sum_{i=1}^{\left\lfloor n\epsilon \right\rfloor} f\left( \frac{\overline{Y}(g)^n_i}{\nu_H}, \frac{\widehat{Y}(g)^n_i}{\nu_H} \right) \\
  \frac{1}{n}\sum_{i=\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor}\left[ f\left( \frac{\overline{Y}(g)^{n}_i}{\nu_H}, \frac{\widehat{Y}(g)^{n}_i}{\nu_H} \right) -  f\left( \frac{\overline{Y}(g)^{n,\epsilon}_i}{\nu_H}, \frac{\widehat{Y}(g)^{n,\epsilon}_i}{\nu_H} \right) \right]
\end{gather}

(5) has that \(f(\cdot)\) is of size \(1\), since \(f\) is assumbed to be of polynomial growth and we showed in ``step 0'' that the arguments are at worst of size \(1\). Then, \(\exists C > 0\) such that \((5) < C\epsilon\). Then, we want to show that as \(n \rightarrow \infty\), \((6) \rightarrow 0\).

MVT reduces (6) to
\begin{gather}
  \frac{1}{n}\sum_{\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} \begin{pmatrix} \partial_1 f(\xi_{1,i}^n) & \partial_2 f(\xi_{2,i}^n) \end{pmatrix} \cdot \begin{pmatrix} \frac{\overline{Y}(g)^{n,\epsilon}_i - \overline{Y}(g)^{n}_i}{\nu_H} & \frac{\widehat{Y}(g)^{n,\epsilon}_i - \widehat{Y}(g)^{n}_i}{\nu_H} \end{pmatrix}
\end{gather}

In particular, we have that the partial derivatives are of size 1, so all that matters is that the differences on the right \(\rightarrow 0\).

For reference/clarity,
\begin{gather}
  \frac{\overline{Y}(g)^{n,\epsilon}_i - \overline{Y}(g)^{n}_i}{\nu_H} \\
  \frac{\widehat{Y}(g)^{n,\epsilon}_i - \widehat{Y}(g)^{n}_i}{\nu_H}
\end{gather}

\begin{align}
  \overline{Y}(g)^{n}_i - \overline{Y}(g)^{n,\epsilon}_i &= \sum_{j=1}^{k_n-1}g^n_i(\Delta_{i+j-1}^{n}Y - \Delta_{i+j-1}^{n,\epsilon}Y) \\
                                                         &= \sum_{j=1}^{k_n-1}g^n_i\int_0^{\frac{i}{n}-\epsilon}\left[ \left( \frac{i+j-1}{n} -s \right)^{H - \frac{1}{2}} - \left( \frac{i+j-1}{n} -s \right)^{H - \frac{1}{2}} \right]\sigma_sdB_s \\
                                                         \intertext{BDG:}
  ||\overline{Y}(g)^{n}_i - \overline{Y}(g)^{n,\epsilon}_i||_{L_p} &\leq \E \left[ \left(\int_0^{\frac{i}{n}-\epsilon}\left[ \cdots \right]^2\sigma_s^2 ds\right)^{\frac{p}{2}} \right]^{\frac{1}{p}} \\
                                                                   &\leq \left(\int_0^{\frac{i}{n}-\epsilon} [\cdots]^2||\sigma_s^2||_{L_{p/2}}ds\right)^{\frac{1}{2}}
                                                                   \intertext{From fBM proof:}
                                                                   \intertext{Since \(\sigma\) is of size 1,}
                                                                   &\leq C_p \left( \int_0^{\frac{i}{n}-\epsilon} \left( \left( \frac{i+l}{n} - s \right)^{H-\frac{1}{2}} - \left( \frac{i+l-1}{n} -s \right)^{H-\frac{1}{2}} \right)^2ds\right) ^{\frac{1}{2}} \\
                                                                   \intertext{Substituting \(r = \frac{i+l-1}{n}-s\),}
                                                                   &= C_p \left( \int_{\epsilon + \frac{l-1}{n}}^{\frac{i+l-1}{n}} \left( \left( r + \frac{1}{n} \right)^{H-\frac{1}{2}} - r^{H-\frac{1}{2}} \right)^2dr\right) ^{\frac{1}{2}} \\
                                                                   &\leq C_p \left( \int_\epsilon^{\infty} \left( \left( r + \frac{1}{n} \right)^{H-\frac{1}{2}} - r^{H-\frac{1}{2}} \right)^2dr\right) ^{\frac{1}{2}} \\
                                                                   \intertext{By MVT,}
                                                                   &\leq C_p \left( \int_\epsilon^{\infty} \left( \frac{1}{n}\left( H-\frac{1}{2} \right)r^{H-\frac{3}{2}} \right)^2dr\right) ^{\frac{1}{2}} \\
                                                                   &\leq C_p \frac{1}{n}\left| H-\frac{1}{2} \right|\left( \int_\epsilon^{\infty} \left( r^{H-\frac{3}{2}} \right)^2dr\right) ^{\frac{1}{2}} \\
                                                                   &= C_\epsilon \frac{1}{n}
\end{align}

Then, considering the size of (10), it is \(\sim k_n \cdot 1 \cdot n^{-1}\).

Next,
\begin{align}
  \widehat{Y}(g)^{n}_i - \widehat{Y}(g)^{n,\epsilon}_i &= \sum_{j=1}^{k_n-1}(\Delta g^n_i)^2((\Delta_{i+j-1}^{n}Y)^2 - (\Delta_{i+j-1}^{n,\epsilon}Y^2) \\
                                                       &= \sum_{j=1}^{k_n-1}(\Delta g^n_i)^2(\Delta_{i+j-1}^{n}Y - \Delta_{i+j-1}^{n,\epsilon}Y)(\Delta_{i+j-1}^{n}Y + \Delta_{i+j-1}^{n,\epsilon}Y) \\
                                                       &\sim k_n \cdot \frac{1}{k_n^2} \cdot n^{-1} \cdot 1 = \frac{1}{k_n \cdot n}
\end{align}

Now we bring back the normalization term \(\nu_H\). Recall:

We will see that we actually need strict inequalities for the upper bounds.

If \( H \in (0, \frac{1}{2})\): (8) has size \(k_n \cdot \frac{1}{n} \cdot \frac{n^H}{\sqrt{k_n}} = \sqrt{k_n} \cdot n^{H - 1} = n^{H - 1 + \frac{\kappa}{2}}\). Since \(\kappa < 2 - 2H\), we have that \(H - 1 + \frac{\kappa}{2} < 0\).

Simiarly, (9) has size \(\frac{1}{k_n \cdot n} \cdot \frac{n^H}{\sqrt{k_n}} = \frac{n^{H-1}}{k_n^{\frac{3}{2}}} = n^{H - 1 - \frac{3}{2}\kappa}\) which is \(o(1)\).

If \( H \in (\frac{1}{2}, 1)\): (8) has size \(k_n \cdot \frac{1}{n} \cdot \frac{n^H}{k_n^H} = k_n^{1-H} \cdot n^{H - 1} = n^{(1 - \kappa)(H - 1)}\). Since \(\kappa < 1\), we have that \(1 - \kappa > 0, H - 1 < 0\).

Simiarly, (9) has size \(\frac{1}{k_n \cdot n} \cdot \frac{n^H}{k_n^H} = \frac{n^{H-1}}{k_n^{H + 1}} = n^{H - 1 - (H + 1)\kappa}\) which is \(o(1)\).

i am hungry :(

Next step is to discretize \(\sigma\) and remove the fBM portion from the second argument; we wish to show that
\begin{equation}
  \E \left[ \left| \frac{1}{n}\sum_{i=\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} \left[ f\left( \frac{\overline{Y}(g)^{n,\epsilon}_i}{\nu_H}, \frac{\widehat{Y}(g)^{n,\epsilon}_i}{\nu_H} \right) - f\left( \frac{\sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i + \rho\overline{Z}(g)^n_i}{\nu_H}, \frac{\rho^2\widehat{Z}(g)^n_i}{\nu_H}\right)\right] \right| \right]
\end{equation}
is bounded by some continuity condition on \(\sigma\), probably dependent on \(\epsilon\).

Via MVT, the difference becomes
\begin{equation}
  \frac{1}{n}\sum_{i=\left\lfloor n\epsilon \right\rfloor + 1}^{\left\lfloor nT \right\rfloor} \begin{pmatrix} \partial_1 f(\xi_{1,i}^n) & \partial_2 f(\xi_{2,i}^n) \end{pmatrix} \cdot \begin{pmatrix} \frac{\overline{X}(g)^{n,\epsilon}_i - \sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i}{\nu_H} & \frac{\widehat{Y}(g)^{n,\epsilon}_i - \rho^2\widehat{Z}(g)^{n}_i}{\nu_H} \end{pmatrix}
\end{equation}

Again, all we care about are the differences:
\begin{gather}
  \frac{\overline{X}(g)^{n,\epsilon}_i - \sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i}{\nu_H} \\
  \frac{\widehat{Y}(g)^{n,\epsilon}_i - \rho^2\widehat{Z}(g)^{n}_i}{\nu_H}
\end{gather}

Handling (25) first, we have that
\begin{align}
  \overline{X}(g)^{n,\epsilon}_i - \sigma_{\frac{i}{n}-\epsilon}\overline{B^H}(g)^{n,\epsilon}_i &= \sum_{j=1}^{k_n-1}g^n_i\left( \Delta_{i+j-1}^{n,\epsilon}X - \sigma_{\frac{i}{n}-\epsilon}\Delta_{i+j-1}^{n,\epsilon}B^H\right) \\
    \lpnorm{\Delta_{i+l}^{n,\epsilon}X - \sigma_{\frac{i}{n}-\epsilon}\Delta_{i+l}^{n,\epsilon}B^H}{2} &= \lpnorm{\int_{\frac{i}{n} - \epsilon}^{\frac{i+l}{n}}\left( \left( \frac{i+l}{n} - s \right)^{H-\frac{1}{2}} - \left( \frac{i+l-1}{n} - s \right)^{H-\frac{1}{2}}\right) (\sigma_s - \sigma_{\frac{i}{n} - \epsilon})dB_s}{2} \\
    \intertext{By Ito's,}
                                                                              &= \left(\int_{\frac{i}{n} - \epsilon}^{\frac{i+l}{n}}\E\left[\left( \left( \frac{i+l}{n} - s \right)^{H-\frac{1}{2}} - \left( \frac{i+l-1}{n} - s \right)^{H-\frac{1}{2}}\right)^2 (\sigma_s - \sigma_{\frac{i}{n} - \epsilon})^2\right]ds\right)^{\frac{1}{2}} \\
                                                                              &= \left(\int_{\frac{i}{n} - \epsilon}^{\frac{i+l}{n}}\left( \left( \frac{i+l}{n} - s \right)^{H-\frac{1}{2}} - \left( \frac{i+l-1}{n} - s \right)^{H-\frac{1}{2}}\right)^2 \E\left[(\sigma_s - \sigma_{\frac{i}{n} - \epsilon})^2\right]ds\right)^{\frac{1}{2}} \\
                                                                              \intertext{Note that for sufficiently large \(n\), \(\frac{l}{n} < \epsilon\), so we have that}
    \E\left[(\sigma_s - \sigma_{\frac{i}{n} - \epsilon})^2\right] &\leq \sup_{\substack{0 \leq r,s \leq T \\ |r-s| \leq 2\epsilon}}\E\left[(\sigma_s - \sigma_r)^2\right] \\
    \intertext{Abbreviate this last supremum to \(S\) and take \(r = \frac{i+l}{n}-s\):}
                                                                  &\leq S^{\frac{1}{2}}\left(\int^{\epsilon+\frac{l}{n}}_0\left(\left(r - \frac{1}{n}\right)^{H-\frac{1}{2}} -  r^{H-\frac{1}{2}}\right)^2 dr\right)^{\frac{1}{2}} \\
                                                                  \intertext{With \(u = nr\),}
                                                                  &= S^{\frac{1}{2}}\left(\int^{n\epsilon+1}_0\left(\left(\frac{u}{n} - \frac{1}{n}\right)^{H-\frac{1}{2}} -  \left( \frac{u}{n} \right)^{H-\frac{1}{2}}\right)^2 n^{-1}du\right)^{\frac{1}{2}} \\
                                                                  &= S^{\frac{1}{2}}n^{-H}\left(\int^{n\epsilon+1}_0\left(\left(u - 1\right)^{H-\frac{1}{2}} -  u^{H-\frac{1}{2}}\right)^2 du\right)^{\frac{1}{2}} \\
                                                                  &\leq S^{\frac{1}{2}}n^{-H}\left(\int^{\infty}_0\left(\left(u - 1\right)^{H-\frac{1}{2}} -  u^{H-\frac{1}{2}}\right)^2 du\right)^{\frac{1}{2}} \\
                                                                  &= CS^{\frac{1}{2}}n^{-H}
\end{align}

PROBLEM! Returning to (27), we get that the \(L_2\) norm will be bounded by \(\sum^{k_n - 1} CS^{\frac{1}{2}}n^{-H} \cdot \nu_H^{-1}\), which \(\rightarrow \infty\) as \(n \rightarrow \infty\) if you fix \(\epsilon\).

\end{document}
